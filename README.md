# TDS_PROJECT1

# ğŸš€ LLM Code Deployment Project

This project implements an **automated buildâ€“deployâ€“revise pipeline** for generating, deploying, and evaluating web applications using Large Language Models (LLMs).  
It hosts an **API endpoint** that accepts structured task requests, uses an LLM to generate code, deploys it to **GitHub Pages**, and notifies an external evaluation service.

---

## ğŸ“Œ Features

- âœ… Secret verification for incoming requests  
- âš¡ Automated app generation using LLMs  
- ğŸŒ GitHub Pages deployment with public repositories  
- ğŸ“ Auto-generated MIT LICENSE and professional README  
- ğŸ“¬ Webhook integration to notify evaluation endpoints  
- ğŸ”„ Round 2 revision support to update apps after instructor feedback

---

## ğŸ§  Project Workflow

### 1ï¸âƒ£ Build Phase
- Receives a task request (JSON) via POST  
- Verifies the student secret  
- Parses brief and attachments  
- Generates a minimal application using LLM  
- Creates a public GitHub repository with a unique name  
- Pushes the code with:
  - `index.html` and required assets  
  - MIT LICENSE at root  
  - Professional README  
- Enables GitHub Pages  
- Posts repo metadata to the evaluation URL within 10 minutes

### 2ï¸âƒ£ Revise Phase
- Receives a **round 2** request to add/modify features  
- Updates the repo accordingly  
- Redeploys to GitHub Pages  
- Notifies the evaluation API again with updated commit and repo details

### 3ï¸âƒ£ Evaluate Phase (Instructor)
- Instructors run automated checks:
  - Static checks (LICENSE, structure, README etc)
  - LLM-based code and documentation quality analysis
  - Dynamic checks using Playwright

---

## ğŸ§° Tech Stack

- **Backend**: Python 3.11, FastAPI  
- **LLM Integration**: OpenAI API, AI Pipe  
- **Deployment**: GitHub REST API 
- **Containerization**: Docker  
- **Testing & Evaluation**: LLM-based analysis

---

## ğŸ³ Running on hugging space at https://arnajit03-tdsp1.hf.space/ (with Docker)
